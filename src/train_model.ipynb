{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "`emg.csv: timestamp, emg_channel_1, emg_channel_2, emg_channel_3, emg_channel_4, emg_channel_5, emg_channel_6, emg_channel_7, emg_channel_8`\n",
    "\n",
    "`finger_angles.csv: timestamp, THUMB, INDEX, MIDDLE, RING, PINKY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from exg.ema import EMA\n",
    "from exg.sma import SMA\n",
    "from exg.iir import IIR\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the JSON data from the file\n",
    "def load_finger_thresholds():\n",
    "    with open('../config/finger_thresholds.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Convert lists back to tuples if necessary\n",
    "    finger_thresholds = tuple(tuple(pair) for pair in data['finger_thresholds'])\n",
    "\n",
    "    return finger_thresholds\n",
    "\n",
    "finger_thresholds = load_finger_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(session_dir: str,\n",
    "              finger_columns: list[str],\n",
    "              window_sizes: list[int],\n",
    "              exg_fs: int):\n",
    "    \"\"\"\n",
    "    Load & process one session directory containing:\n",
    "      - exg.csv      (timestamp + raw EMG channels)\n",
    "      - prompt.csv   (timestamp + finger columns)\n",
    "    \"\"\"\n",
    "    # Read CSVs ---\n",
    "    exg_path    = os.path.join(session_dir, \"exg.csv\")\n",
    "    prompt_path = os.path.join(session_dir, \"prompt.csv\")\n",
    "    exg_data    = pd.read_csv(exg_path)\n",
    "    angle_data  = pd.read_csv(prompt_path)\n",
    "\n",
    "    # Infer column sets ---\n",
    "    raw_columns    = [c for c in exg_data.columns   if c != \"timestamp\"]\n",
    "    # only pick the finger columns you care about:\n",
    "    angle_columns  = [c for c in finger_columns    if c in angle_data.columns]\n",
    "\n",
    "    # Timestamps to timedelta ---\n",
    "    exg_data['timestamp']   = pd.to_timedelta(exg_data[\"timestamp\"], unit='ms')\n",
    "    angle_data['timestamp'] = pd.to_timedelta(angle_data[\"timestamp\"], unit='ms')\n",
    "\n",
    "    # Apply filters\n",
    "    iir_proc = IIR(\n",
    "        num_channels=len(raw_columns),\n",
    "        fs=exg_fs,\n",
    "        lowpass_fs=exg_fs/2,\n",
    "        highpass_fs=10,\n",
    "        notch_fs_list=[50, 60],\n",
    "        filter_order=4\n",
    "    )\n",
    "    filtered = iir_proc.process(exg_data[raw_columns].values)\n",
    "    exg_data[raw_columns] = filtered\n",
    "\n",
    "    # Apply smoothing methods\n",
    "    ema_proc = EMA(window_sizes=window_sizes,\n",
    "                   num_channels=len(raw_columns),\n",
    "                   fs=exg_fs)\n",
    "    sma_proc = SMA(window_sizes=window_sizes,\n",
    "                   num_channels=len(raw_columns),\n",
    "                   fs=exg_fs)\n",
    "\n",
    "    ema_vals = ema_proc.process(filtered)\n",
    "    ema_df   = ema_proc.results_to_df(ema_vals)\n",
    "\n",
    "    sma_vals = sma_proc.process(filtered)\n",
    "    sma_df   = sma_proc.results_to_df(sma_vals)\n",
    "\n",
    "    # Add all columns to exg_data\n",
    "    processed_exg = pd.concat([exg_data, ema_df, sma_df], axis=1)\n",
    "\n",
    "    # Shift timestamps to allow user to transition after the prompt\n",
    "    transition_ms = 2000\n",
    "    angle_data['timestamp'] += pd.Timedelta(milliseconds=transition_ms)\n",
    "    tolerance_ms = 5000 - transition_ms\n",
    "\n",
    "    merged = pd.merge_asof(\n",
    "        processed_exg.sort_values('timestamp'),\n",
    "        angle_data[angle_columns + [\"timestamp\"]].sort_values('timestamp'),\n",
    "        on=\"timestamp\",\n",
    "        direction='backward',\n",
    "        tolerance=pd.Timedelta(milliseconds=tolerance_ms)\n",
    "    ).set_index(\"timestamp\")\n",
    "    merged = merged.dropna()\n",
    "\n",
    "    return merged, raw_columns, ema_df.columns.tolist(), sma_df.columns.tolist(), sma_proc.window_intervals_ms\n",
    "\n",
    "\n",
    "def load_session(session_folder: str,\n",
    "                 finger_columns: list[str],\n",
    "                 window_sizes: list[int],\n",
    "                 exg_fs: int):\n",
    "    \"\"\"\n",
    "    Discover all r_<n> subfolders in session_folder, load & process each.\n",
    "    Returns:\n",
    "      - sessions: list of DataFrames\n",
    "      - ema_columns, sma_columns, window_intervals_ms (from last session)\n",
    "    \"\"\"\n",
    "    # find all r_<n> dirs\n",
    "    dirs = [\n",
    "        d for d in os.listdir(session_folder)\n",
    "        if os.path.isdir(os.path.join(session_folder, d))\n",
    "           and re.match(r\"^r_(\\d+)$\", d)\n",
    "    ]\n",
    "    # sort by the numeric idx\n",
    "    dirs.sort(key=lambda d: int(d.split(\"_\", 1)[1]))\n",
    "\n",
    "    sessions = []\n",
    "    ema_cols = sma_cols = None\n",
    "    window_ints = None\n",
    "\n",
    "    for d in dirs:\n",
    "        full_path = os.path.join(session_folder, d)\n",
    "        print(f\"Loading session {d} …\")\n",
    "        df, raw_cols, ema_cols, sma_cols, window_ints = load_data(\n",
    "            session_dir    = full_path,\n",
    "            finger_columns = finger_columns,\n",
    "            window_sizes   = window_sizes,\n",
    "            exg_fs         = exg_fs\n",
    "        )\n",
    "        sessions.append(df)\n",
    "\n",
    "    return sessions, raw_cols, ema_cols, sma_cols, window_ints\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Main script\n",
    "# -----------------\n",
    "\n",
    "window_sizes   = [64]\n",
    "finger_columns = [\"thumb\", \"index\", \"middle\", \"pinky\"]\n",
    "session_folder = \"../data/s_4_26_25\"    # contains r_0/, r_1/, …\n",
    "exg_fs         = 125\n",
    "\n",
    "sessions, exg_cols, ema_cols, sma_cols, win_intervals = load_session(\n",
    "    session_folder,\n",
    "    finger_columns,\n",
    "    window_sizes,\n",
    "    exg_fs,\n",
    ")\n",
    "\n",
    "# sessions is now a list of one DataFrame per r_<n> folder\n",
    "for i, df in enumerate(sessions):\n",
    "    print(f\"Session {i}: {df.shape[0]} rows, columns = {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Average Value and Standard Deviation for the first data session\n",
    "def print_stats(data):\n",
    "    \"\"\"Prints the average and standard deviation of the data.\"\"\"\n",
    "    for column in data.columns:\n",
    "        print(f\"{column}: (MEAN: {data[column].mean()}, STD: {data[column].std()})\")\n",
    "        \n",
    "print_stats(sessions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph_emg_channel(emg_channel, title=\"EMG Channel\"):\n",
    "    \"\"\"Graphs a single EMG channel.\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(emg_channel.abs())\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "    \n",
    "def graph_emg(emg_data, emg_columns=None, title=\"EMG\"):\n",
    "    \"\"\"Graphs the EMG data for all channels in a 2x4 grid.\"\"\"\n",
    "    # If no columns are specified, use all columns except the timestamp\n",
    "    if emg_columns is None:\n",
    "        emg_columns = emg_data.columns\n",
    "    \n",
    "    # Plot a 2 x 4 grid of EMG data\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.suptitle(title)\n",
    "    for i, channel_name in enumerate(emg_columns):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.plot(emg_data[channel_name], label=f\"{channel_name}\")\n",
    "        plt.title(f\"{channel_name}\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def graph_finger(finger_data, finger_channels=None, title=\"Finger Angles\"):\n",
    "    \"\"\"Graphs the finger angle data for all fingers in a 2x3 grid.\"\"\"\n",
    "    # If no columns are specified, use all columns except the timestamp\n",
    "    if finger_channels is None:\n",
    "        finger_channels = finger_data.columns\n",
    "    \n",
    "    # Plot a 2 x 3 grid of finger data\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.suptitle(title)\n",
    "    for i, finger_name in enumerate(finger_channels):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        plt.plot(finger_data[finger_name], label=f\"{finger_name}\")\n",
    "        plt.title(f\"{finger_name}\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Angle\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Graph the first subsession\n",
    "graph_emg(sessions[-1][sma_cols[:8]], title=\"EMG Channel 1\")\n",
    "graph_emg(sessions[-1][sma_cols[:8]], title=\"EMG Channel 1\")\n",
    "graph_finger(sessions[-1][finger_columns], title=\"Finger Angles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_fingers(data, finger_columns, finger_thresholds):\n",
    "    \"\"\"Preprocesses the finger angle data by normalizing it.\"\"\"\n",
    "    data_preprocessed = data.copy()\n",
    "    for i, finger in enumerate(finger_columns):\n",
    "        finger_threshold = finger_thresholds[i]\n",
    "        data_preprocessed[finger] = (data_preprocessed[finger] - finger_threshold[0]) / (finger_threshold[1] - finger_threshold[0])\n",
    "        data_preprocessed[finger] = data_preprocessed[finger].clip(0, 1)\n",
    "        \n",
    "    \n",
    "    # Lowpass filter the finger data\n",
    "    # window_s = 0.1\n",
    "    # window_frames = int(125 * window_s)\n",
    "    # data_preprocessed[finger_columns] = data_preprocessed[finger_columns].rolling(window=window_frames).mean()\n",
    "        \n",
    "    return data_preprocessed\n",
    "\n",
    "def preprocess_fingers_session(session, finger_columns, finger_thresholds):\n",
    "    \"\"\"Preprocesses the finger angle data for a given session.\"\"\"\n",
    "    return [preprocess_fingers(data, finger_columns, finger_thresholds) for data in session]\n",
    "    \n",
    "# Preprocess the finger data\n",
    "preprocessed = preprocess_fingers_session(sessions, finger_columns, finger_thresholds)\n",
    "\n",
    "graph_finger(preprocessed[-1][finger_columns], title=\"Preprocessed Finger Angles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan_session(session):\n",
    "    \"\"\"Drops NaN values from all data in a session.\"\"\"\n",
    "    return [data.dropna() for data in session]\n",
    "\n",
    "# Drop NaN values from the session\n",
    "preprocessed = drop_nan_session(preprocessed)\n",
    "# Print the number of samples before and after dropping NaN values\n",
    "for i, data in enumerate(sessions):\n",
    "    print(f\"Samples before dropping NaN for session {i}: {len(data)}\")\n",
    "    print(f\"Samples after dropping NaN for session {i}: {len(preprocessed[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "x_columns = [ema_cols[i] for i in [0, 1, 5, 6, 8, 9, 11, 12]] # Best EMG channels: [0, 1, 5, 6, 8, 9, 11, 12]\n",
    "num_sessions = len(preprocessed)\n",
    "preprocessed_range = list(range(0, num_sessions))\n",
    "val_sessions = preprocessed_range[-1:]\n",
    "# Set to the difference of the preprocessed range and the validation sessions\n",
    "train_sessions = list(set(preprocessed_range) - set(val_sessions))\n",
    "\n",
    "delay = 4 # Delay in frames\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for session_idx in train_sessions:\n",
    "    emg = preprocessed[session_idx][x_columns]\n",
    "    finger = preprocessed[session_idx][finger_columns]\n",
    "    X_train.append(emg[:emg.shape[0]-delay])\n",
    "    y_train.append(finger[delay:])\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "for session_idx in val_sessions:\n",
    "    emg = preprocessed[session_idx][x_columns]\n",
    "    finger = preprocessed[session_idx][finger_columns]\n",
    "    X_val.append(emg[:emg.shape[0]-delay])\n",
    "    y_val.append(finger[delay:])\n",
    "    \n",
    "X_val_no_delay = []\n",
    "y_val_no_delay = []\n",
    "for session_idx in val_sessions:\n",
    "    emg = preprocessed[session_idx][x_columns]\n",
    "    finger = preprocessed[session_idx][finger_columns]\n",
    "    X_val_no_delay.append(emg)\n",
    "    y_val_no_delay.append(finger)\n",
    "    \n",
    "# Combine the training and validation sets\n",
    "X_train = np.concatenate(X_train)\n",
    "X_val = np.concatenate(X_val)\n",
    "y_train = np.concatenate(y_train)\n",
    "y_val = np.concatenate(y_val)\n",
    "X_val_no_delay = np.concatenate(X_val_no_delay)\n",
    "y_val_no_delay = np.concatenate(y_val_no_delay)\n",
    "\n",
    "# Mean and standard deviation for each channel\n",
    "train_mean = X_train.mean(axis=0)\n",
    "train_std = X_train.std(axis=0)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = (X_train - train_mean) / train_std\n",
    "X_val = (X_val - train_mean) / train_std\n",
    "X_val_no_delay = (X_val_no_delay - train_mean) / train_std\n",
    "# X_train = X_train / train_std\n",
    "# X_val = X_val / train_std\n",
    "# X_val_no_delay = X_val_no_delay / train_std\n",
    "\n",
    "# Print the shapes of the training and validation sets\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, Y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GaussianNoise, Input, BatchNormalization\n",
    "\n",
    "def create_model(input_shape, output_shape, gaussian_noise=0.1, loss='mse', metrics=['r2_score', 'binary_accuracy']):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        GaussianNoise(gaussian_noise),\n",
    "        # Dense(128, activation='relu'),\n",
    "        # BatchNormalization(),\n",
    "        # Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(output_shape, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, patience=10, epochs=200, batch_size=1024):\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_r2_score', \n",
    "                                   patience=patience, \n",
    "                                   restore_best_weights=True,\n",
    "                                   mode='max')\n",
    "                                   \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.5, \n",
    "                                  patience=5, \n",
    "                                  verbose=1,\n",
    "                                  mode='min'\n",
    "                                  )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=epochs, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks=[early_stopping, reduce_lr],\n",
    "                        verbose=1)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def create_and_train_model(X_train, y_train, X_val, y_val, gaussian_noise=0.01,\n",
    "                           patience=10, epochs=200, batch_size=1024,\n",
    "                           loss='BinaryCrossentropy', metrics=['r2_score', 'binary_accuracy']):\n",
    "    # Create the model\n",
    "    # if loss == 'categorical_crossentropy':\n",
    "    #     y_train = np.round(y_train)\n",
    "    #     y_val = np.round(y_val)\n",
    "    input_shape = (X_train.shape[1],)\n",
    "    output_shape = y_train.shape[1]\n",
    "\n",
    "    model = create_model(input_shape=input_shape, \n",
    "                         output_shape=output_shape, \n",
    "                         gaussian_noise=gaussian_noise,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    return train_model(model, X_train, y_train, X_val, y_val, \n",
    "                       patience=patience, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = create_and_train_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_loss, val_r2, val_b_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "if val_r2 > 0:\n",
    "    print(f\"Validation R Score: {sqrt(val_r2)}\")\n",
    "print(f\"Validation R2 Score: {val_r2}\")\n",
    "print(f\"Validation Binary Accuracy: {val_b_acc}\")\n",
    "# Calculate and print the correlation coefficient for each finger\n",
    "for i, finger in enumerate(finger_columns):\n",
    "    r2 = r2_score(y_val[:, i], y_pred[:, i])\n",
    "    if r2 < 0:\n",
    "        r2 = 0\n",
    "    print(f\"R score for {finger}: {sqrt(r2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = session_folder + \"/models/model_0\"\n",
    "\n",
    "# Create the models directory if it does not exist\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model.save(f\"{model_dir}/model.keras\")\n",
    "\n",
    "np.save(f'{model_dir}/window_sizes.npy', np.array(window_sizes))\n",
    "np.save(f'{model_dir}/std.npy', train_std)\n",
    "np.save(f'{model_dir}/ema_columns.npy', np.array(ema_cols), allow_pickle=True)\n",
    "np.save(f'{model_dir}/sma_columns.npy', np.array(sma_cols), allow_pickle=True)\n",
    "np.save(f'{model_dir}/finger_columns.npy', np.array(finger_columns), allow_pickle=True)\n",
    "np.save(f'{model_dir}/finger_thresholds.npy', np.array(finger_thresholds), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which are the best 8 (or n) Channels\n",
    "Takes very long to run, only run if you want to determine the most important n important channels, such as to reduce the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_subset(ch_idx, X_train_full, y_train, X_val_full, y_val,\n",
    "                    gaussian_noise=0.01, patience=10, epochs=200, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Train on only the columns in ch_idx, return the best val_r2_score.\n",
    "    \"\"\"\n",
    "    X_tr = X_train_full[:, ch_idx]\n",
    "    X_vl = X_val_full[:, ch_idx]\n",
    "    \n",
    "    # re-train model from scratch\n",
    "    model, history = create_and_train_model(\n",
    "        X_tr, y_train, X_vl, y_val,\n",
    "        gaussian_noise=gaussian_noise,\n",
    "        patience=patience,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        loss='BinaryCrossentropy',\n",
    "        metrics=['r2_score','binary_accuracy']\n",
    "    )\n",
    "    # get max val_r2_score achieved\n",
    "    return max(history.history['val_r2_score'])\n",
    "\n",
    "def select_best_channels_greedy(channel_indices, X_train_full, y_train,\n",
    "                                 X_val_full, y_val, n_channels, **eval_kwargs):\n",
    "    \"\"\"\n",
    "    Greedy forward selection: start empty, add one channel at a time.\n",
    "    \n",
    "    Args:\n",
    "      channel_indices: list of ints [0,1,2,…,C-1]\n",
    "      X_*: 2D arrays (samples × channels)\n",
    "      y_*: labels\n",
    "      n_channels: how many to pick\n",
    "      eval_kwargs: passed to evaluate_subset\n",
    "    \n",
    "    Returns:\n",
    "      selected: list of channel indices (length n_channels)\n",
    "      scores: list of val_r2 after each addition\n",
    "    \"\"\"\n",
    "    remaining = set(channel_indices)\n",
    "    selected = []\n",
    "    scores = []\n",
    "    \n",
    "    for k in tqdm(range(n_channels), desc=\"Selecting channels\"):\n",
    "        best_chan, best_score = None, -np.inf\n",
    "        for c in remaining:\n",
    "            trial = selected + [c]\n",
    "            score = evaluate_subset(trial, X_train_full, y_train,\n",
    "                                    X_val_full, y_val, **eval_kwargs)\n",
    "            if score > best_score:\n",
    "                best_score, best_chan = score, c\n",
    "        selected.append(best_chan)\n",
    "        remaining.remove(best_chan)\n",
    "        scores.append(best_score)\n",
    "        print(f\"Picked channel {best_chan} → val_r2={best_score:.4f}\")\n",
    "    return selected, scores\n",
    "\n",
    "# — example usage —\n",
    "# suppose X_train (N×16), X_val (M×16), y_train (N×D), y_val (M×D):\n",
    "all_ch = list(range(X_train.shape[1]))\n",
    "best8, perf8 = select_best_channels_greedy(\n",
    "    all_ch, X_train, y_train, X_val, y_val,\n",
    "    n_channels=8,\n",
    "    gaussian_noise=0.01,\n",
    "    patience=5,   # fewer epochs for speed, tweak as you like\n",
    "    epochs=100,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "print(\"Best 8 channels:\", best8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Results on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred = (model.predict(X_val_no_delay) > 0.5) * 1\n",
    "y_test_df = pd.DataFrame(y_val_no_delay, columns=finger_columns)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=finger_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display\n",
    "import plotly.io as pio\n",
    "\n",
    "def create_plotly_animation(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    finger_names, \n",
    "    window_size=250*5, \n",
    "    step_size=10, \n",
    "    fps=10, \n",
    "    max_fingers=5,\n",
    "    external_window=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates an efficient animated Plotly figure with multiple subplots, each representing a finger.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: pandas DataFrame containing actual target values.\n",
    "    - y_pred: pandas DataFrame containing predicted target values.\n",
    "    - finger_names: List of finger names (columns in DataFrame).\n",
    "    - window_size: Number of samples to display per frame.\n",
    "    - step_size: Number of samples to move forward in each frame.\n",
    "    - fps: Frames per second for the animation.\n",
    "    - max_fingers: Maximum number of fingers (subplots) to plot simultaneously.\n",
    "    - external_window: If True, opens the animation in a separate browser window.\n",
    "    \n",
    "    Returns:\n",
    "    - Plotly Figure object.\n",
    "    \"\"\"\n",
    "    # Limit the number of fingers to plot for efficiency\n",
    "    if len(finger_names) > max_fingers:\n",
    "        print(f\"Limiting to first {max_fingers} fingers for performance.\")\n",
    "        finger_names = finger_names[:max_fingers]\n",
    "        y_true = y_true[finger_names]\n",
    "        y_pred = y_pred[finger_names]\n",
    "    \n",
    "    num_fingers = len(finger_names)\n",
    "    num_samples = len(y_true)\n",
    "    \n",
    "    # Calculate the number of frames\n",
    "    frames_indices = list(range(0, num_samples - window_size + step_size, step_size))\n",
    "    if not frames_indices:\n",
    "        frames_indices = [0]\n",
    "    \n",
    "    # Create subplots: one row per finger\n",
    "    fig = make_subplots(\n",
    "        rows=num_fingers, \n",
    "        cols=1, \n",
    "        shared_xaxes=True,\n",
    "        subplot_titles=finger_names,\n",
    "        vertical_spacing=0.05\n",
    "    )\n",
    "    \n",
    "    # Initialize traces for each finger\n",
    "    for i, finger in enumerate(finger_names, start=1):\n",
    "        # Actual values\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[], \n",
    "                y=[],\n",
    "                mode='lines',\n",
    "                line=dict(color='blue'),\n",
    "                name=f'{finger} - Actual',\n",
    "                showlegend=(i == 1)  # Show legend only for the first subplot\n",
    "            ),\n",
    "            row=i, \n",
    "            col=1\n",
    "        )\n",
    "        # Predicted values\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[], \n",
    "                y=[],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', dash='dash'),\n",
    "                name=f'{finger} - Predicted',\n",
    "                showlegend=(i == 1)  # Show legend only for the first subplot\n",
    "            ),\n",
    "            row=i, \n",
    "            col=1\n",
    "        )\n",
    "        \n",
    "        # Fix the y-axis range between 0 and 1\n",
    "        fig.update_yaxes(range=[-0.05, 1.05], row=i, col=1)\n",
    "    \n",
    "    # Generate frames\n",
    "    frames = []\n",
    "    for start in frames_indices:\n",
    "        end = start + window_size\n",
    "        if end > num_samples:\n",
    "            end = num_samples\n",
    "            start = max(end - window_size, 0)\n",
    "        frame_data = []\n",
    "        x_range = list(range(end - start))\n",
    "        for i, finger in enumerate(finger_names):\n",
    "            # Actual data\n",
    "            actual = y_true.iloc[start:end, i].values\n",
    "            # Predicted data\n",
    "            pred = y_pred.iloc[start:end, i].values\n",
    "            # Append updated traces\n",
    "            frame_data.append(go.Scatter(x=x_range, y=actual))\n",
    "            frame_data.append(go.Scatter(x=x_range, y=pred))\n",
    "        frames.append(go.Frame(data=frame_data, name=str(start)))\n",
    "    \n",
    "    fig.frames = frames\n",
    "    \n",
    "    # Add play and pause buttons\n",
    "    fig.update_layout(\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                type='buttons',\n",
    "                showactive=False,\n",
    "                y=1,\n",
    "                x=1.05,\n",
    "                xanchor='right',\n",
    "                yanchor='top',\n",
    "                pad=dict(t=0, r=10),\n",
    "                buttons=[\n",
    "                    dict(\n",
    "                        label='Play',\n",
    "                        method='animate',\n",
    "                        args=[\n",
    "                            None,\n",
    "                            dict(\n",
    "                                frame=dict(duration=1000/fps, redraw=True),\n",
    "                                transition=dict(duration=0),\n",
    "                                fromcurrent=True,\n",
    "                                mode='immediate'\n",
    "                            )\n",
    "                        ]\n",
    "                    ),\n",
    "                    dict(\n",
    "                        label='Pause',\n",
    "                        method='animate',\n",
    "                        args=[\n",
    "                            [None],\n",
    "                            dict(\n",
    "                                frame=dict(duration=0, redraw=False),\n",
    "                                transition=dict(duration=0),\n",
    "                                mode='immediate'\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        sliders=[\n",
    "            dict(\n",
    "                active=0,\n",
    "                currentvalue=dict(prefix='Start Index: '),\n",
    "                pad=dict(t=50, b=10),\n",
    "                steps=[\n",
    "                    dict(\n",
    "                        method='animate',\n",
    "                        args=[\n",
    "                            [str(start)],\n",
    "                            dict(\n",
    "                                frame=dict(duration=1000/fps, redraw=True),\n",
    "                                transition=dict(duration=0),\n",
    "                                mode='immediate'\n",
    "                            )\n",
    "                        ],\n",
    "                        label=str(start)\n",
    "                    ) for start in frames_indices\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        height=300*num_fingers, \n",
    "        width=900, \n",
    "        showlegend=True,\n",
    "        title='Real-Time Animation of Predictions vs Actual Values'\n",
    "    )\n",
    "    \n",
    "    # Set initial data\n",
    "    if frames:\n",
    "        initial_start = frames_indices[0]\n",
    "        initial_end = initial_start + window_size\n",
    "        if initial_end > num_samples:\n",
    "            initial_end = num_samples\n",
    "            initial_start = max(initial_end - window_size, 0)\n",
    "        x_initial = list(range(initial_end - initial_start))\n",
    "        for i, finger in enumerate(finger_names):\n",
    "            actual = y_true.iloc[initial_start:initial_end, i].values\n",
    "            pred = y_pred.iloc[initial_start:initial_end, i].values\n",
    "            fig['data'][2*i].x = x_initial\n",
    "            fig['data'][2*i].y = actual\n",
    "            fig['data'][2*i+1].x = x_initial\n",
    "            fig['data'][2*i+1].y = pred\n",
    "    \n",
    "    # Display the figure\n",
    "    if external_window:\n",
    "        # Use 'browser' renderer to open the animation in a separate browser window\n",
    "        pio.renderers.default = 'browser'\n",
    "    else:\n",
    "        # Use the default renderer (typically inline in Jupyter)\n",
    "        pio.renderers.default = 'notebook'\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "window_size = 100    # Number of samples per frame\n",
    "step_size = int(250 * 1/10)       # Step size for each frame\n",
    "fps = 10             # Frames per second\n",
    "max_fingers = 5      # Adjust based on your system's capability\n",
    "\n",
    "# Create the Plotly animation\n",
    "plotly_fig = create_plotly_animation(\n",
    "    y_true=y_test_df, \n",
    "    y_pred=y_pred_df, \n",
    "    finger_names=finger_columns, \n",
    "    window_size=window_size, \n",
    "    step_size=step_size, \n",
    "    fps=fps, \n",
    "    max_fingers=max_fingers\n",
    ")\n",
    "\n",
    "# Display the figure in Jupyter Notebook\n",
    "plotly_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAV, WL, WA, and VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_MAV(signal_window):\n",
    "    \"\"\"Mean of the Absolute Value\"\"\"\n",
    "    return np.mean(np.abs(signal_window))\n",
    "\n",
    "def compute_WL(signal_window):\n",
    "    \"\"\"Waveform Length\"\"\"\n",
    "    return np.sum(np.abs(np.diff(signal_window)))\n",
    "\n",
    "def compute_WA(signal_window, threshold=0.02):\n",
    "    \"\"\"Willison Amplitude (count of threshold crossings)\"\"\"\n",
    "    return np.sum(np.abs(signal_window) > threshold)\n",
    "\n",
    "def compute_VAR(signal_window):\n",
    "    \"\"\"Variance\"\"\"\n",
    "    return np.var(signal_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Layer, GaussianNoise, Conv1D, Flatten, MaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "\n",
    "# -----------------------------\n",
    "# Create Sequences from Data\n",
    "# -----------------------------\n",
    "def create_sequences(X, y, window_size=64, step_size=5):\n",
    "    \"\"\"\n",
    "    Creates overlapping sequences from the data.\n",
    "\n",
    "    Parameters:\n",
    "    - X: np.ndarray, shape (num_samples, num_features)\n",
    "    - y: np.ndarray, shape (num_samples, num_targets)\n",
    "    - window_size: int, number of time steps per sequence\n",
    "    - step_size: int, step size between sequences\n",
    "\n",
    "    Returns:\n",
    "    - X_seq: np.ndarray, shape (num_sequences, window_size, num_features)\n",
    "    - y_seq: np.ndarray, shape (num_sequences, num_targets)\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for start in range(0, len(X) - window_size, step_size):\n",
    "        end = start + window_size\n",
    "        X_seq.append(X[start:end])\n",
    "        y_seq.append(y[end-1])  # Predicting the last target in the window\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def create_sequences_from_session(preprocessed, session_list, exg_columns, window_size=64, step_size=5):\n",
    "    \"\"\"\n",
    "    Creates sequences from a given session.\n",
    "\n",
    "    Parameters:\n",
    "    - session: list of pd.DataFrame, each containing EMG and finger data\n",
    "    - window_size: int, number of time steps per sequence\n",
    "    - step_size: int, step size between sequences\n",
    "\n",
    "    Returns:\n",
    "    - X_seq: np.ndarray, shape (num_sequences, window_size, num_features)\n",
    "    - y_seq: np.ndarray, shape (num_sequences, num_targets)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    # include all raw exg channels\n",
    "    for i in session_list:\n",
    "        emg_data = preprocessed[i][exg_columns]\n",
    "        finger_data = preprocessed[i][finger_columns]\n",
    "        X.append(emg_data)\n",
    "        y.append(finger_data)\n",
    "\n",
    "    X = pd.concat(X)\n",
    "    y = pd.concat(y)\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    \n",
    "    # Mean and STD\n",
    "    std = X.std(axis=0)\n",
    "    X = X / std\n",
    "\n",
    "    return create_sequences(X, y, window_size, step_size)\n",
    "\n",
    "# Define window size and step size\n",
    "WINDOW_SIZE = 64  # Adjust based on your data characteristics\n",
    "STEP_SIZE = 1    # Adjust based on desired overlap\n",
    "\n",
    "# Create sequences for training, validation, and test sets\n",
    "X_train_seq, y_train_seq = create_sequences_from_session(preprocessed, train_sessions, exg_columns=exg_cols, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
    "X_val_seq, y_val_seq = create_sequences_from_session(preprocessed, val_sessions, exg_columns=exg_cols, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
    "\n",
    "print(\"Training sequences shape:\", X_train_seq.shape, y_train_seq.shape)\n",
    "print(\"Validation sequences shape:\", X_val_seq.shape, y_val_seq.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Define Attention Layer\n",
    "# -----------------------------\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Custom Attention Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape: (batch_size, timesteps, features)\n",
    "        self.W = self.add_weight(name='att_weight', \n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='random_normal', \n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', \n",
    "                                 shape=(input_shape[-1],),  # Corrected shape\n",
    "                                 initializer='zeros', \n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Compute attention scores\n",
    "        # scores shape: (batch_size, timesteps, features)\n",
    "        scores = K.tanh(K.dot(inputs, self.W) + self.b)  # Shape: (batch_size, timesteps, features)\n",
    "        attention_weights = K.softmax(scores, axis=1)    # Shape: (batch_size, timesteps, features)\n",
    "        # Apply attention weights\n",
    "        context_vector = attention_weights * inputs       # Shape: (batch_size, timesteps, features)\n",
    "        context_vector = K.sum(context_vector, axis=1)   # Shape: (batch_size, features)\n",
    "        return context_vector\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "# -----------------------------\n",
    "# Define the model\n",
    "# -----------------------------\n",
    "\n",
    "def create_cnn(input_shape, output_shape, gaussian_noise=0.1, loss='binary_crossentropy', metrics=['r2_score']):\n",
    "    \"\"\"\n",
    "    Creates an attention-based sequence model.\n",
    "\n",
    "    Parameters:\n",
    "    - input_shape: tuple, shape of the input (timesteps, features)\n",
    "    - output_shape: int, number of target outputs\n",
    "    - gaussian_noise: float, standard deviation for Gaussian noise\n",
    "    - loss: string, loss function\n",
    "    - metrics: list, list of metrics\n",
    "\n",
    "    Returns:\n",
    "    - model: compiled Keras model\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)  # (timesteps, features)\n",
    "    x = GaussianNoise(gaussian_noise)(inputs)\n",
    "    \n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = MaxPool1D(pool_size=4)(x)\n",
    "    \n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = MaxPool1D(pool_size=4)(x)\n",
    "    \n",
    "    # Attention Layer\n",
    "    # x = AttentionLayer()(x)\n",
    "    \n",
    "    # Flatten the output of the CNN\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    output = Dense(output_shape, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Train the Model\n",
    "# -----------------------------\n",
    "\n",
    "# Define input and output shapes\n",
    "input_shape = (WINDOW_SIZE, X_train_seq.shape[2])  # (timesteps, features)\n",
    "output_shape = y_train_seq.shape[1]                # Number of finger angles\n",
    "\n",
    "# Create the model\n",
    "model = create_cnn(\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    gaussian_noise=0.1,\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['r2_score']\n",
    ")\n",
    "\n",
    "# Display the model architecture\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    batch_size=1024,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate the Model\n",
    "# -----------------------------\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_loss, val_r2 = model.evaluate(X_val_seq, y_val_seq, verbose=0)\n",
    "y_pred_val = model.predict(X_val_seq)\n",
    "\n",
    "print(f\"\\nValidation Loss: {val_loss:.4f}, Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "# Calculate and print the R score for each finger on validation set\n",
    "print(\"\\nValidation R Scores per Finger:\")\n",
    "for i, finger in enumerate(finger_columns):\n",
    "    r2 = r2_score(y_val_seq[:, i], y_pred_val[:, i])\n",
    "    r = sqrt(max(r2, 0))  # Ensure non-negative before sqrt\n",
    "    print(f\"{finger}: R = {r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
